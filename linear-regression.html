<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Aayush | Linear Regression Blog</title>
<style>
  body { margin: 0; font-family: Arial, sans-serif; background: #fafafa; color: #222; }
  .navbar { display: flex; justify-content: flex-start; gap: 32px; background-color: #333; padding: 14px 32px; position: fixed; top: 0; width: 100%; z-index: 100; }
  .navbar a { color: white; text-decoration: none; font-weight: bold; padding: 8px 16px; border-radius: 4px; transition: background 0.3s; }
  .navbar a:hover { background-color: #555; color: white; }

  section { padding: 120px 32px 80px; max-width: 900px; margin: auto; }
  h1, h2, h3 { margin-bottom: 12px; }
  p { line-height: 1.6; }

  pre {
    background: #eee;
    padding: 16px;
    overflow-x: auto;
    border-radius: 8px;
    border: 1px solid #ccc;
  }
</style>
</head>

<body>

<div class="navbar">
  <a href="index.html">Home</a>
  <a href="projects.html">Projects</a>
  <a href="about.html">About Me</a>
</div>

<section>
  <h1>Linear Regression Tutorial</h1>
  <p>
    In this post, I worked through a Python linear regression tutorial and recorded my notes along with answers to the questions from the notebook.
  </p>

  <h2>Introduction</h2>
  <p>
    Linear regression is a method to fit a straight line to a dataset and predict one variable from another.  
    In this tutorial, I explored ordinary least squares regression, residuals, and total least squares regression.
  </p>

  <h2>Loading the Data</h2>
  <pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Read the dataset
anscombe_i = pd.read_csv('anscombe_i.csv')
anscombe_i.head()</code></pre>

  <h2>Plotting the Data</h2>
  <pre><code>plt.scatter(anscombe_i.x, anscombe_i.y, color='black')
plt.xlabel("X")
plt.ylabel("Y")
plt.title("Scatter Plot of Dataset")
plt.show()</code></pre>

  <h2>Fitting a Linear Regression Line</h2>
  <pre><code>from sklearn.linear_model import LinearRegression

X = anscombe_i.x.values.reshape(-1,1)
y = anscombe_i.y.values.reshape(-1,1)

model = LinearRegression()
model.fit(X, y)

print("Slope (m):", model.coef_[0])
print("Intercept (b):", model.intercept_)

plt.scatter(X, y, color='black')
plt.plot(X, model.predict(X), color='green')
plt.show()</code></pre>

  <h2>Residuals</h2>
  <pre><code>residuals = y - model.predict(X)
plt.scatter(X, residuals)
plt.xlabel("X")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()</code></pre>

  <h2>Notes & Answers</h2>

  <h3>1. What is `m` in y = mx + b?</h3>
  <p>
    `m` is the slope of the line, showing how much y changes for a unit change in x.
  </p>

  <h3>2. What is the R squared?</h3>
  <p>
    R squared shows how well the line fits the data. Closer to 1 means the line explains most of the variance in y.
  </p>

  <h3>3. What is the Adj. R squared?</h3>
  <p>
    Adjusted R squared accounts for number of predictors and prevents artificial inflation when more variables are added.
  </p>

  <h3>4. What is the P value?</h3>
  <p>
    P value tells if the slope is statistically significant. A small value (≤ 0.05) means the slope is meaningful.
  </p>

  <h3>5. What are the 95% confidence intervals?</h3>
  <p>
    The 95% confidence intervals give a range where the true slope and intercept are expected to fall 95% of the time.
  </p>

  <h2>View the Full Notebook</h2>
  <p>
    You can check out the full Python notebook here: 
    <a href="https://github.com/781148/Aayush-Khosla-websit/blob/main/linear-regression-tutorial.ipynb" target="_blank">
      Linear Regression Notebook on GitHub
    </a>
  </p>
</section>

<div class="footer">© 2025 Aayush</div>
</body>
</html>
